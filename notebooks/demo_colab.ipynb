{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e03b32c0",
      "metadata": {},
      "source": [
        "# OllaBridge Ã— Google Colab â€” Production Demo (4 Scenarios)\n",
        "\n",
        "**Updated:** 2026-01-06  \n",
        "This notebook shows four endâ€‘toâ€‘end setups for running LLM inference where the GPU lives **(Colab GPU, your local GPU, or a registered node in the cloud)** while keeping a single OpenAIâ€‘compatible API surface.\n",
        "\n",
        "Weâ€™ll use **OllaBridge Cloud** at:\n",
        "\n",
        "- **Cloud base:** `https://cloud.ollabridge.com`\n",
        "\n",
        "---\n",
        "\n",
        "## Before you start\n",
        "\n",
        "### What is what?\n",
        "- **Gateway / Control Plane (OllaBridge):** the OpenAI-compatible API you call (`/v1/...`), handles auth + routing.\n",
        "- **Node / Worker (ollabridge-node):** a machine that provides compute (usually runs Ollama + models).\n",
        "- **OllaBridge Cloud :** a hosted control plane + secure relay for nodes behind NAT (no port forwarding).\n",
        "\n",
        "### Security defaults used in this notebook\n",
        "- If you donâ€™t configure `API_KEYS`, OllaBridge generates a **per-run secret key** and prints it.\n",
        "- **By default it does NOT write keys to `.env`.**\n",
        "- If you *explicitly* want persistence for dev only: `ollabridge start --write-env`\n",
        "\n",
        "> **Production guidance:** Use environment variables or a secret manager for API keys; avoid committing `.env` files.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0e0351c",
      "metadata": {},
      "source": [
        "## Architecture diagrams\n",
        "\n",
        "### 1) High-level routing\n",
        "\n",
        "```mermaid\n",
        "flowchart LR\n",
        "  Client[Client (Colab / Local)] -->|OpenAI-compatible HTTP| GW[OllaBridge Gateway]\n",
        "  GW -->|routes requests| Node[Worker Node(s)]\n",
        "  Node -->|Ollama / model inference| GPU[(GPU/CPU)]\n",
        "```\n",
        "\n",
        "### 2) Cloud relay (no port forwarding)\n",
        "\n",
        "```mermaid\n",
        "sequenceDiagram\n",
        "  participant C as Client (Colab)\n",
        "  participant Cloud as cloud.ollabridge.com\n",
        "  participant Dev as Your Device/Node\n",
        "  C->>Cloud: HTTPS request (OpenAI /v1)\n",
        "  Cloud->>Dev: WebSocket relay (req)\n",
        "  Dev->>Cloud: WebSocket response (res/delta/done)\n",
        "  Cloud->>C: HTTPS response (stream or JSON)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4215649e",
      "metadata": {},
      "source": [
        "---\n",
        "# â˜ï¸ Scenario 3: OllaBridge Cloud Relay (Local â†’ Cloud â†’ Colab)\n",
        "\n",
        "**Goal:** Securely access your local machine (GPU) from Colab **without port forwarding**, using **OllaBridge Cloud** as a relay.\n",
        "\n",
        "**Cloud endpoint:** `https://cloud.ollabridge.com`\n",
        "\n",
        "## How it works\n",
        "1. Your **local computer** runs Ollama + your model and connects **outbound** to OllaBridge Cloud over WebSocket (`ollabridge-node connect`).\n",
        "2. This notebook (Colab) sends OpenAI-compatible requests to **https://cloud.ollabridge.com/v1**.\n",
        "3. OllaBridge Cloud relays the request to your device and returns the response (optionally streaming).\n",
        "\n",
        "```mermaid\n",
        "flowchart LR\n",
        "  Colab[Google Colab] -->|HTTPS /v1| Cloud[cloud.ollabridge.com]\n",
        "  Cloud -->|WebSocket relay| Local[Your local node (GPU)]\n",
        "  Local --> Ollama[Ollama + model]\n",
        "```\n",
        "\n",
        "## On your local computer (one-time registration + connect)\n",
        "Run these commands on your local machine:\n",
        "\n",
        "```bash\n",
        "pip install -U ollabridge\n",
        "ollama pull deepseek-r1  # ensure model exists locally\n",
        "\n",
        "# Register the device with OllaBridge Cloud (may show a pairing URL/code)\n",
        "ollabridge-node register --cloud https://cloud.ollabridge.com\n",
        "\n",
        "# Keep the node connected (relay channel)\n",
        "ollabridge-node connect --cloud https://cloud.ollabridge.com\n",
        "```\n",
        "\n",
        "> After the node is connected, use your **Cloud API key** (`CLOUD_API_KEY`) from the OllaBridge Cloud dashboard in this notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec522b1c",
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Variables (edit these)\n",
        "import os\n",
        "\n",
        "MODEL_ID = os.environ.get(\"MODEL_ID\", \"deepseek-r1\")\n",
        "\n",
        "# Scenario 2: Colab -> your local gateway (paste values when you have them)\n",
        "LOCAL_GATEWAY_BASE_URL = os.environ.get(\"LOCAL_GATEWAY_BASE_URL\", \"\")  # e.g. https://xxxx.tunnel.../v1\n",
        "LOCAL_API_KEY = os.environ.get(\"LOCAL_API_KEY\", \"\")\n",
        "\n",
        "# Scenarios 3/4: OllaBridge Cloud\n",
        "CLOUD_BASE_URL = os.environ.get(\"CLOUD_BASE_URL\", \"https://cloud.ollabridge.com/v1\")\n",
        "CLOUD_API_KEY = os.environ.get(\"CLOUD_API_KEY\", \"\")  # paste from Cloud dashboard or your gateway banner\n",
        "\n",
        "print(\"MODEL_ID:\", MODEL_ID)\n",
        "print(\"CLOUD_BASE_URL:\", CLOUD_BASE_URL)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c65edfb1",
      "metadata": {},
      "source": [
        "### Helper utilities\n",
        "\n",
        "These helpers keep the notebook readable:\n",
        "- `health_check(base_url)`\n",
        "- `list_models(base_url, api_key)`\n",
        "- `chat(base_url, api_key, model, message)`\n",
        "- `chat_stream(base_url, api_key, model, message)` (best-effort streaming)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7f2510e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import time\n",
        "import requests\n",
        "from typing import Optional, Iterable\n",
        "\n",
        "def _auth_headers(api_key: str) -> dict:\n",
        "    return {\n",
        "        \"Authorization\": f\"Bearer {api_key}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "    }\n",
        "\n",
        "def health_check(base_url: str, timeout: int = 5) -> bool:\n",
        "    if not base_url:\n",
        "        print(\"âŒ base_url is empty.\")\n",
        "        return False\n",
        "    # normalize: base_url might be .../v1\n",
        "    health_url = base_url.replace(\"/v1\", \"\") + \"/health\"\n",
        "    try:\n",
        "        r = requests.get(health_url, timeout=timeout)\n",
        "        ok = (r.status_code == 200)\n",
        "        print((\"âœ…\" if ok else \"âŒ\"), \"GET\", health_url, \"->\", r.status_code)\n",
        "        if not ok:\n",
        "            print(r.text[:400])\n",
        "        return ok\n",
        "    except Exception as e:\n",
        "        print(\"âŒ health_check error:\", type(e).__name__, str(e)[:200])\n",
        "        return False\n",
        "\n",
        "def list_models(base_url: str, api_key: str, timeout: int = 15) -> list[str]:\n",
        "    url = base_url.rstrip(\"/\") + \"/models\"\n",
        "    r = requests.get(url, headers=_auth_headers(api_key), timeout=timeout)\n",
        "    r.raise_for_status()\n",
        "    data = r.json()\n",
        "    return [m.get(\"id\") for m in data.get(\"data\", []) if m.get(\"id\")]\n",
        "\n",
        "def chat(base_url: str, api_key: str, model: str, message: str, timeout: int = 90) -> str:\n",
        "    url = base_url.rstrip(\"/\") + \"/chat/completions\"\n",
        "    payload = {\n",
        "        \"model\": model,\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": message}],\n",
        "        \"temperature\": 0.3,\n",
        "    }\n",
        "    r = requests.post(url, headers=_auth_headers(api_key), json=payload, timeout=timeout)\n",
        "    r.raise_for_status()\n",
        "    j = r.json()\n",
        "    return j[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "def chat_stream(base_url: str, api_key: str, model: str, message: str, timeout: int = 90) -> Iterable[str]:\n",
        "    \"\"\"Best-effort streaming for OpenAI-compatible SSE-style responses.\n",
        "\n",
        "    If your gateway doesn't support streaming, it may return a normal JSON response.\n",
        "    \"\"\"\n",
        "    url = base_url.rstrip(\"/\") + \"/chat/completions\"\n",
        "    payload = {\n",
        "        \"model\": model,\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": message}],\n",
        "        \"temperature\": 0.3,\n",
        "        \"stream\": True,\n",
        "    }\n",
        "    with requests.post(url, headers=_auth_headers(api_key), json=payload, stream=True, timeout=timeout) as r:\n",
        "        r.raise_for_status()\n",
        "        # Some implementations stream as lines; some return JSON once.\n",
        "        content_type = r.headers.get(\"Content-Type\", \"\")\n",
        "        if \"application/json\" in content_type:\n",
        "            j = r.json()\n",
        "            yield j[\"choices\"][0][\"message\"][\"content\"]\n",
        "            return\n",
        "\n",
        "        for raw in r.iter_lines(decode_unicode=True):\n",
        "            if not raw:\n",
        "                continue\n",
        "            line = raw.strip()\n",
        "            if line.startswith(\"data:\"):\n",
        "                line = line[len(\"data:\"):].strip()\n",
        "            if line == \"[DONE]\":\n",
        "                break\n",
        "            try:\n",
        "                j = json.loads(line)\n",
        "                delta = j[\"choices\"][0].get(\"delta\", {}).get(\"content\")\n",
        "                if delta:\n",
        "                    yield delta\n",
        "            except Exception:\n",
        "                # Not JSON; ignore\n",
        "                continue\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e6130ca",
      "metadata": {},
      "source": [
        "# Scenario 1 â€” Run OllaBridge *inside* Google Colab (use Colab GPU)\n",
        "\n",
        "### Goal\n",
        "Run everything in Colab:\n",
        "- Ollama (model runtime)\n",
        "- OllaBridge Gateway (OpenAI-compatible API)\n",
        "- Inference happens on **Colab GPU**\n",
        "\n",
        "### Notes (Colab specifics)\n",
        "- Colab does not run systemd; we start `ollama serve` in the background.\n",
        "- The first model pull can take time depending on the model size.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2c3aefa",
      "metadata": {},
      "source": [
        "## 1.1 Install Ollama + OllaBridge (Colab)\n",
        "\n",
        "Run this once per runtime.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13392a0b",
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m pip -q install -U ollabridge openai requests\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09b1a29e",
      "metadata": {},
      "source": [
        "## 1.2 Start Ollama server (background)\n",
        "\n",
        "We start `ollama serve` in the background and wait until it answers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4485497",
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess, shlex, sys, re\n",
        "\n",
        "# Start ollama serve in background (idempotent-ish)\n",
        "_ = subprocess.Popen(\n",
        "    [\"bash\", \"-lc\", \"nohup ollama serve > /tmp/ollama.log 2>&1 &\"],\n",
        "    stdout=subprocess.DEVNULL,\n",
        "    stderr=subprocess.DEVNULL,\n",
        ")\n",
        "\n",
        "# Wait for Ollama\n",
        "for i in range(30):\n",
        "    try:\n",
        "        r = requests.get(\"http://localhost:11434/api/tags\", timeout=2)\n",
        "        if r.status_code == 200:\n",
        "            print(\"âœ… Ollama is up\")\n",
        "            break\n",
        "    except Exception:\n",
        "        pass\n",
        "    time.sleep(1)\n",
        "else:\n",
        "    print(\"âŒ Ollama did not start. Check logs:\")\n",
        "    !tail -n 50 /tmp/ollama.log\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afce12dd",
      "metadata": {},
      "source": [
        "## 1.3 Pull a model\n",
        "\n",
        "Choose a model that fits your Colab GPU quota.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72320da8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pull the model (example: deepseek-r1)\n",
        "!ollama pull {MODEL_ID}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ffed245",
      "metadata": {},
      "source": [
        "## 1.4 Start OllaBridge gateway (Colab)\n",
        "\n",
        "This exposes the OpenAI-compatible API at:\n",
        "- `http://localhost:11435/v1`\n",
        "\n",
        "OllaBridge will print an API key in the banner. Copy it into `COLAB_API_KEY` below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da6fef2d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start the gateway in the background so the notebook continues\n",
        "_ = subprocess.Popen(\n",
        "    [\"bash\", \"-lc\", \"nohup ollabridge start --host 0.0.0.0 --port 11435 --log-level info > /tmp/ollabridge.log 2>&1 &\"],\n",
        "    stdout=subprocess.DEVNULL,\n",
        "    stderr=subprocess.DEVNULL,\n",
        ")\n",
        "\n",
        "# Show last lines to grab the key\n",
        "!tail -n 60 /tmp/ollabridge.log\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "199b740d",
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Paste the API key printed by OllaBridge (Scenario 1)\n",
        "COLAB_GATEWAY_BASE_URL = \"http://localhost:11435/v1\"\n",
        "COLAB_API_KEY = \"\"  # paste from banner, e.g. sk-ollabridge-...\n",
        "\n",
        "print(\"Base URL:\", COLAB_GATEWAY_BASE_URL)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "509076ea",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Smoke test: health + chat\n",
        "assert health_check(COLAB_GATEWAY_BASE_URL), \"Gateway is not healthy yet.\"\n",
        "\n",
        "print(\"Available models (first 10):\")\n",
        "try:\n",
        "    print(list_models(COLAB_GATEWAY_BASE_URL, COLAB_API_KEY)[:10])\n",
        "except Exception as e:\n",
        "    print(\"Models listing failed (auth key wrong?):\", type(e).__name__, str(e)[:200])\n",
        "\n",
        "print(\"\\nChat result:\")\n",
        "print(chat(COLAB_GATEWAY_BASE_URL, COLAB_API_KEY, MODEL_ID, \"Say hello from Colab GPU in one sentence.\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e3b37bc",
      "metadata": {},
      "source": [
        "# Scenario 2 â€” Use your *local* GPU from Google Colab\n",
        "\n",
        "### Goal\n",
        "Run the gateway on your computer (using your GPU), and call it from Colab.\n",
        "\n",
        "### How it works\n",
        "1. On your computer: start OllaBridge (which uses your local Ollama + GPU).\n",
        "2. Expose it to the internet safely:\n",
        "   - Quick dev: `ollabridge start --share`\n",
        "   - Production: VPN / reverse proxy / **OllaBridge Cloud** (Scenario 3)\n",
        "\n",
        "Then Colab calls the **public base URL**.\n",
        "\n",
        "> If you started locally with `--share`, your base URL will look like `https://.../v1`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88f994c0",
      "metadata": {},
      "source": [
        "## 2.1 On your computer (outside Colab)\n",
        "\n",
        "Run:\n",
        "\n",
        "```bash\n",
        "ollabridge start --share\n",
        "```\n",
        "\n",
        "Copy from the banner:\n",
        "- Public URL (ends with `/v1`)\n",
        "- API key\n",
        "\n",
        "Paste them below in this notebook (`LOCAL_GATEWAY_BASE_URL`, `LOCAL_API_KEY`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a7e63fa",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scenario 2: health + chat\n",
        "if not LOCAL_GATEWAY_BASE_URL or not LOCAL_API_KEY:\n",
        "    print(\"â„¹ï¸ Paste LOCAL_GATEWAY_BASE_URL and LOCAL_API_KEY in the Variables cell above.\")\n",
        "else:\n",
        "    ok = health_check(LOCAL_GATEWAY_BASE_URL)\n",
        "    if ok:\n",
        "        print(\"Chat result (runs on your local GPU):\")\n",
        "        print(chat(LOCAL_GATEWAY_BASE_URL, LOCAL_API_KEY, MODEL_ID, \"Confirm this response is generated on my local GPU.\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "134c3df9",
      "metadata": {},
      "source": [
        "# Scenario 3 â€” Register your local computer in OllaBridge Cloud and call it from Colab\n",
        "\n",
        "### Goal\n",
        "No tunnels. No port forwarding.\n",
        "Your local computer (GPU) connects **outbound** to the cloud using WebSockets.\n",
        "Colab sends requests to the cloud, and the cloud relays them to your computer.\n",
        "\n",
        "**Cloud:** `https://cloud.ollabridge.com`\n",
        "\n",
        "---\n",
        "\n",
        "## 3.1 On your local computer (outside Colab)\n",
        "\n",
        "1) Install and run the cloud-connected node (example commands; use your exact CLI if it differs):\n",
        "\n",
        "```bash\n",
        "pip install -U ollabridge\n",
        "# register device (you may be prompted to complete a pairing step)\n",
        "ollabridge-node register --cloud https://cloud.ollabridge.com\n",
        "# connect and stay connected\n",
        "ollabridge-node connect --cloud https://cloud.ollabridge.com\n",
        "```\n",
        "\n",
        "2) Ensure the node has access to Ollama + the model:\n",
        "```bash\n",
        "ollama pull deepseek-r1\n",
        "```\n",
        "\n",
        "> If your cloud uses a pairing code (like TV login), follow the printed URL/code once during registration.\n",
        "\n",
        "---\n",
        "\n",
        "## 3.2 From Colab: call the cloud base URL\n",
        "\n",
        "Paste your cloud API key into `CLOUD_API_KEY` above (from your Cloud dashboard or gateway settings).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ca8ede0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scenario 3: health + chat via cloud\n",
        "if not CLOUD_API_KEY:\n",
        "    print(\"â„¹ï¸ Paste CLOUD_API_KEY in the Variables cell above.\")\n",
        "else:\n",
        "    ok = health_check(CLOUD_BASE_URL)\n",
        "    if ok:\n",
        "        print(\"Chat result (cloud relays to your registered local GPU node):\")\n",
        "        print(chat(CLOUD_BASE_URL, CLOUD_API_KEY, MODEL_ID, \"Hello! Reply with one sentence to confirm relay through OllaBridge cloud.\"))\n",
        "\n",
        "        print(\"\\nStreaming demo (best-effort):\")\n",
        "        out = []\n",
        "        for chunk in chat_stream(CLOUD_BASE_URL, CLOUD_API_KEY, MODEL_ID, \"Stream 10 short words separated by spaces.\"):\n",
        "            out.append(chunk)\n",
        "            sys.stdout.write(chunk)\n",
        "            sys.stdout.flush()\n",
        "        print(\"\\n\\nDone.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efdfbad4",
      "metadata": {},
      "source": [
        "---\n",
        "# ðŸ–¥ï¸ Scenario 4: Register Google Colab as a Cloud Node (Laptop â†’ Cloud â†’ Colab GPU)\n",
        "\n",
        "**Goal:** Turn this Colab runtime into a **worker node** registered in **OllaBridge Cloud**, so you can use **Colab GPU** from your local computer.\n",
        "\n",
        "**Cloud endpoint:** `https://cloud.ollabridge.com`\n",
        "\n",
        "## How it works\n",
        "- Colab connects outbound to the Cloud (no inbound ports).\n",
        "- Your laptop (or a local gateway) calls the Cloud API (`https://cloud.ollabridge.com/v1`).\n",
        "- Cloud relays requests to this Colab node.\n",
        "\n",
        "```mermaid\n",
        "flowchart LR\n",
        "  Laptop[Your laptop / client] -->|HTTPS /v1| Cloud[cloud.ollabridge.com]\n",
        "  Cloud -->|WebSocket relay| Colab[This Colab node (GPU)]\n",
        "  Colab --> Ollama[Ollama + model]\n",
        "```\n",
        "\n",
        "## Important note\n",
        "Colab runtimes are **ephemeral**. If the runtime disconnects, the node goes offline and must reconnect/register again.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "437f375a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install (if not already)\n",
        "!python -m pip -q install -U ollabridge\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59cd4715",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Register this Colab runtime as a node (you may be prompted for pairing)\n",
        "# NOTE: If your cloud requires browser-based pairing, follow the printed instructions.\n",
        "!ollabridge-node register --cloud https://cloud.ollabridge.com\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e922fe93",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Connect and keep alive (run this cell and leave it running if needed)\n",
        "# In Colab, long-running processes can be started in the background as well.\n",
        "import subprocess, time\n",
        "\n",
        "_ = subprocess.Popen(\n",
        "    [\"bash\", \"-lc\", \"nohup ollabridge-node connect --cloud https://cloud.ollabridge.com > /tmp/ollabridge_node.log 2>&1 &\"],\n",
        "    stdout=subprocess.DEVNULL,\n",
        "    stderr=subprocess.DEVNULL,\n",
        ")\n",
        "print(\"âœ… Node connect started. Tail logs:\")\n",
        "!tail -n 80 /tmp/ollabridge_node.log\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02545769",
      "metadata": {},
      "source": [
        "## 4.2 On your local computer: route requests to the Colab node through the cloud\n",
        "\n",
        "Depending on your product mode, you will do one of the following:\n",
        "\n",
        "### Option A â€” Call cloud directly (recommended for demos)\n",
        "Set your client:\n",
        "- `base_url = https://cloud.ollabridge.com/v1`\n",
        "- `api_key = <your CLOUD_API_KEY>`\n",
        "\n",
        "The cloud will route to the node you select/own (often via device selection in dashboard).\n",
        "\n",
        "### Option B â€” Run a local gateway that routes via cloud (if your CLI supports it)\n",
        "Example (your CLI may differ):\n",
        "```bash\n",
        "ollabridge start --cloud https://cloud.ollabridge.com\n",
        "```\n",
        "\n",
        "Then your local apps call:\n",
        "- `http://localhost:11435/v1`\n",
        "\n",
        "---\n",
        "\n",
        "### Minimal local client example (Python)\n",
        "```python\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "  base_url=\"https://cloud.ollabridge.com/v1\",\n",
        "  api_key=\"YOUR_CLOUD_KEY\",\n",
        ")\n",
        "\n",
        "resp = client.chat.completions.create(\n",
        "  model=\"deepseek-r1\",\n",
        "  messages=[{\"role\":\"user\",\"content\":\"Run this on Colab GPU\"}],\n",
        ")\n",
        "print(resp.choices[0].message.content)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1702eeb1",
      "metadata": {},
      "source": [
        "# Troubleshooting\n",
        "\n",
        "### 401 Unauthorized\n",
        "- Ensure your client uses the **same** API key the gateway/cloud expects.\n",
        "- For local gateway: the key is printed at startup and is set via `API_KEYS`.\n",
        "- For cloud: use the API key from your cloud account/dashboard.\n",
        "\n",
        "### Model not found\n",
        "- Ensure the model exists on the worker machine:\n",
        "  - `ollama pull <model>`\n",
        "- Ensure you request the correct model id (`MODEL_ID` in this notebook).\n",
        "\n",
        "### Cloud relay connected but requests donâ€™t route\n",
        "- Confirm your node is **connected** and **owned/authorized** for your cloud API key.\n",
        "- Check device selection/routing rules in the cloud dashboard (if applicable).\n",
        "\n",
        "---\n",
        "\n",
        "# Production checklist (quick)\n",
        "- Use env vars or secret manager for `API_KEYS` / cloud secrets\n",
        "- Use HTTPS everywhere\n",
        "- Restrict CORS origins\n",
        "- Rate limit pairing/login endpoints (cloud)\n",
        "- Rotate keys regularly\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdc67d01",
      "metadata": {},
      "source": [
        "\n",
        "## Using Colab GPU from your laptop (after Scenario 4 is connected)\n",
        "\n",
        "Once this Colab node is **Connected** in OllaBridge Cloud, you can use it from your laptop in two common ways:\n",
        "\n",
        "### A) Call OllaBridge Cloud directly (simplest)\n",
        "- Base URL: `https://cloud.ollabridge.com/v1`\n",
        "- API key: your `CLOUD_API_KEY`\n",
        "\n",
        "```python\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(base_url=\"https://cloud.ollabridge.com/v1\", api_key=\"YOUR_CLOUD_API_KEY\")\n",
        "resp = client.chat.completions.create(\n",
        "    model=\"deepseek-r1\",\n",
        "    messages=[{\"role\":\"user\",\"content\":\"Run this on Colab GPU via the cloud relay.\"}],\n",
        ")\n",
        "print(resp.choices[0].message.content)\n",
        "```\n",
        "\n",
        "### B) Optional: run a local gateway and route to cloud (if supported)\n",
        "Some deployments support a local gateway that routes to Cloud:\n",
        "\n",
        "```bash\n",
        "ollabridge start --cloud https://cloud.ollabridge.com\n",
        "```\n",
        "\n",
        "Then call:\n",
        "- `http://localhost:11435/v1`\n",
        "\n",
        "> If your current CLI doesnâ€™t support `--cloud`, use option A.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    },
    "title": "OllaBridge Ã— Google Colab â€” 4 Scenarios (MatrixHub Cloud)"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
