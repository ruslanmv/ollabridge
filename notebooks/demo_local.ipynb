{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bea5a49a",
   "metadata": {},
   "source": [
    "# OllaBridge — Local Deployment Demo (Enterprise-Style)\n",
    "\n",
    "This notebook is a **hands-on, copy/paste-ready** tutorial for using a **local OllaBridge gateway** (OpenAI-compatible API) with clean, production-grade examples.\n",
    "\n",
    "**What you’ll do**\n",
    "- Start the gateway locally with **auto-reload**\n",
    "- Verify the service (health + model readiness)\n",
    "- Call **Chat Completions** with the official `openai` Python SDK\n",
    "- Use **streaming** responses\n",
    "- Apply **enterprise hygiene**: configuration, secrets, timeouts, retries, and logging\n",
    "\n",
    "> **Assumptions**\n",
    "> - You already started OllaBridge (or you will in Step 1) and see output similar to:\n",
    ">\n",
    "> `... start --host 0.0.0.0 --port 11435 --reload ...`\n",
    ">\n",
    "> with:\n",
    "> - Local API: `http://localhost:11435/v1`\n",
    "> - Health: `http://localhost:11435/health`\n",
    "> - Key: `sk-ollabridge-...`\n",
    "\n",
    "**Last updated:** 2026-01-05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8a2444",
   "metadata": {},
   "source": [
    "## 0) Prerequisites\n",
    "\n",
    "### Required\n",
    "- Python **3.10+**\n",
    "- A running **OllaBridge** gateway (local)\n",
    "- A model available in your gateway (example below uses `deepseek-r1`)\n",
    "\n",
    "### Install client dependencies (this notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98183e31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T23:18:32.739741Z",
     "iopub.status.busy": "2026-01-05T23:18:32.739238Z",
     "iopub.status.idle": "2026-01-05T23:18:45.547316Z",
     "shell.execute_reply": "2026-01-05T23:18:45.545085Z"
    }
   },
   "outputs": [],
   "source": [
    "# If you're running this notebook locally:\n",
    "# python -m pip install -U openai requests python-dotenv\n",
    "\n",
    "!python -m pip -q install -U openai requests python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626ac9b8",
   "metadata": {},
   "source": [
    "## 1) Start the gateway (auto-reload)\n",
    "\n",
    "Run this command **in a terminal** (not inside the notebook) from your OllaBridge repo/venv:\n",
    "\n",
    "```bash\n",
    ".venv/bin/python3 -m ollabridge.cli.main start --host 0.0.0.0 --port 11435 --reload --log-level info\n",
    "```\n",
    "\n",
    "When it’s ready you should see a banner like:\n",
    "\n",
    "- Local API: `http://localhost:11435/v1`\n",
    "- Health: `http://localhost:11435/health`\n",
    "- Key: `sk-ollabridge-...`\n",
    "\n",
    "### Security note (enterprise)\n",
    "Treat the key like a secret:\n",
    "- do **not** commit it to git\n",
    "- prefer environment variables (or a secrets manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283294aa",
   "metadata": {},
   "source": [
    "## 2) Configure the client (base URL + API key)\n",
    "\n",
    "You can provide the key in either header style:\n",
    "- `X-API-Key: sk-...`\n",
    "- `Authorization: Bearer sk-...`\n",
    "\n",
    "The `openai` Python SDK uses `Authorization: Bearer ...` automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f98231a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T23:18:45.555287Z",
     "iopub.status.busy": "2026-01-05T23:18:45.554528Z",
     "iopub.status.idle": "2026-01-05T23:18:45.562034Z",
     "shell.execute_reply": "2026-01-05T23:18:45.560512Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base URL: http://localhost:11435/v1\n",
      "API key set: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ✅ Load variables from .env file into the environment\n",
    "# This defaults to looking for a file named \".env\" in the current directory\n",
    "load_dotenv() \n",
    "\n",
    "# Now os.getenv will find the values defined in your .env file\n",
    "OLLAS_BASE_URL = os.getenv(\"OLLAS_BASE_URL\", \"http://localhost:11435/v1\")\n",
    "OLLAS_API_KEY  = os.getenv(\"OLLAS_API_KEY\", \"sk-ollabridge-REPLACE_ME\")\n",
    "\n",
    "print(\"Base URL:\", OLLAS_BASE_URL)\n",
    "# Check if key is loaded (and not the default placeholder)\n",
    "print(\"API key set:\", OLLAS_API_KEY.startswith(\"sk-ollabridge-\") and \"REPLACE_ME\" not in OLLAS_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f09e353e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T23:18:45.566061Z",
     "iopub.status.busy": "2026-01-05T23:18:45.565627Z",
     "iopub.status.idle": "2026-01-05T23:18:45.726142Z",
     "shell.execute_reply": "2026-01-05T23:18:45.712584Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def gateway_health(base_url: str, timeout_s: int = 10):\n",
    "    \"\"\"Return (ok: bool, payload_or_error: object).\"\"\"\n",
    "    try:\n",
    "        health_url = base_url.replace(\"/v1\", \"\") + \"/health\"\n",
    "        r = requests.get(health_url, timeout=timeout_s)\n",
    "        r.raise_for_status()\n",
    "        return True, r.json()\n",
    "    except Exception as e:\n",
    "        return False, str(e)\n",
    "\n",
    "def require_gateway(base_url: str):\n",
    "    ok, info = gateway_health(base_url, timeout_s=10)\n",
    "    if ok:\n",
    "        print(\"✅ Gateway reachable:\", base_url)\n",
    "        print(\"Health:\", info)\n",
    "        return True\n",
    "    print(\"⚠️ Gateway NOT reachable:\", base_url)\n",
    "    print(\"Reason:\", info)\n",
    "    print(\"\\nTip: start OllaBridge first, or set the correct base URL / tunnel URL.\")\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6debff6c",
   "metadata": {},
   "source": [
    "## 3) Health check (fast validation)\n",
    "\n",
    "This should return HTTP 200 and a simple JSON payload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cd272cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T23:18:45.731329Z",
     "iopub.status.busy": "2026-01-05T23:18:45.730998Z",
     "iopub.status.idle": "2026-01-05T23:18:45.740248Z",
     "shell.execute_reply": "2026-01-05T23:18:45.738885Z"
    }
   },
   "outputs": [],
   "source": [
    "# Health check (safe): won't crash the notebook if the gateway isn't up yet.\n",
    "health_url = OLLAS_BASE_URL.replace(\"/v1\", \"\") + \"/health\"\n",
    "ok, info = gateway_health(OLLAS_BASE_URL, timeout_s=20)\n",
    "if ok:\n",
    "    info\n",
    "else:\n",
    "    print(\"Skipping health check because gateway is not reachable.\")\n",
    "    print(\"Reason:\", info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a0dc04",
   "metadata": {},
   "source": [
    "## 4) List models (optional)\n",
    "\n",
    "If your gateway supports the standard OpenAI-compatible endpoint, this will show the models it exposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df01c3a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T23:18:45.744594Z",
     "iopub.status.busy": "2026-01-05T23:18:45.744286Z",
     "iopub.status.idle": "2026-01-05T23:18:45.752155Z",
     "shell.execute_reply": "2026-01-05T23:18:45.750870Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Gateway reachable: http://localhost:11435/v1\n",
      "Health: {'status': 'ok', 'mode': 'gateway', 'default_model': 'llama3', 'detail': 'runtimes=1'}\n"
     ]
    }
   ],
   "source": [
    "if not require_gateway(OLLAS_BASE_URL):\n",
    "    print('⏭️ Skipping this cell (gateway not reachable).')\n",
    "else:\n",
    "    from openai import OpenAI\n",
    "\n",
    "    client = OpenAI(\n",
    "        base_url=OLLAS_BASE_URL,\n",
    "        api_key=OLLAS_API_KEY,\n",
    "    )\n",
    "\n",
    "    # Some gateways support /models; if yours doesn't, skip this cell.\n",
    "    try:\n",
    "        models = client.models.list()\n",
    "        [m.id for m in models.data][:10]\n",
    "    except Exception as e:\n",
    "        print(\"Model listing not available (this is OK). Error:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1bb246",
   "metadata": {},
   "source": [
    "## 5) Chat Completions (basic)\n",
    "\n",
    "This is the core OpenAI-compatible usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f11d4c91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T23:18:45.755920Z",
     "iopub.status.busy": "2026-01-05T23:18:45.755650Z",
     "iopub.status.idle": "2026-01-05T23:18:45.767688Z",
     "shell.execute_reply": "2026-01-05T23:18:45.765986Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Gateway reachable: http://localhost:11435/v1\n",
      "Health: {'status': 'ok', 'mode': 'gateway', 'default_model': 'llama3', 'detail': 'runtimes=1'}\n",
      "Here are 3 bullet points on why gateways are useful in enterprise AI deployments:\n",
      "\n",
      "*   **Security Enforcement:** Gateways can enforce security policies (like authentication, authorization, and data masking) at the edge or between systems, protecting sensitive AI models and data.\n",
      "*   **Interoperability & Integration:** They act as intermediaries, translating data formats and handling communication between diverse systems (on-prem, cloud, SaaS) and AI platforms.\n",
      "*   **Controlled Access:** Gateways provide a centralized point to manage and restrict access to internal AI infrastructure or proprietary models, preventing unauthorized exposure.\n"
     ]
    }
   ],
   "source": [
    "if not require_gateway(OLLAS_BASE_URL):\n",
    "    print('⏭️ Skipping this cell (gateway not reachable).')\n",
    "else:\n",
    "    model_name = os.getenv(\"OLLAS_MODEL\", \"deepseek-r1\")\n",
    "\n",
    "    resp = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a concise assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Give me 3 bullet points on why gateways are useful in enterprise AI deployments.\"},\n",
    "        ],\n",
    "        temperature=0.2,\n",
    "    )\n",
    "\n",
    "    print(resp.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d554d91",
   "metadata": {},
   "source": [
    "## 6) Streaming responses (production-friendly UX)\n",
    "\n",
    "Streaming is ideal for:\n",
    "- chat UIs\n",
    "- long outputs\n",
    "- faster perceived latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a13554ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T23:18:45.829619Z",
     "iopub.status.busy": "2026-01-05T23:18:45.829198Z",
     "iopub.status.idle": "2026-01-05T23:18:45.838874Z",
     "shell.execute_reply": "2026-01-05T23:18:45.837242Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Gateway reachable: http://localhost:11435/v1\n",
      "Health: {'status': 'ok', 'mode': 'gateway', 'default_model': 'llama3', 'detail': 'runtimes=1'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if not require_gateway(OLLAS_BASE_URL):\n",
    "    print('⏭️ Skipping this cell (gateway not reachable).')\n",
    "else:\n",
    "    stream = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Answer in a short paragraph.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Explain what auto-reload means for a local API gateway in development.\"},\n",
    "        ],\n",
    "        temperature=0.2,\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    out = []\n",
    "    for event in stream:\n",
    "        delta = event.choices[0].delta\n",
    "        if delta and delta.content:\n",
    "            out.append(delta.content)\n",
    "            print(delta.content, end=\"\", flush=True)\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f168a99",
   "metadata": {},
   "source": [
    "## 7) Robust client wrapper (timeouts, retries, structured errors)\n",
    "\n",
    "Below is a small helper you can copy into production services.\n",
    "\n",
    "**Why?**\n",
    "- predictable timeouts\n",
    "- transparent retry policy\n",
    "- consistent logging and error handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3eaec727",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T23:18:45.842790Z",
     "iopub.status.busy": "2026-01-05T23:18:45.842486Z",
     "iopub.status.idle": "2026-01-05T23:18:45.853206Z",
     "shell.execute_reply": "2026-01-05T23:18:45.852102Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Gateway reachable: http://localhost:11435/v1\n",
      "Health: {'status': 'ok', 'mode': 'gateway', 'default_model': 'llama3', 'detail': 'runtimes=1'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11435/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:ollabridge-demo:chat completion ok (3.90s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An API gateway is an intermediary server acting as a single entry point for clients to access multiple backend services, handling routing, security, rate limiting, and other cross-cutting concerns.\n"
     ]
    }
   ],
   "source": [
    "if not require_gateway(OLLAS_BASE_URL):\n",
    "    print('⏭️ Skipping this cell (gateway not reachable).')\n",
    "else:\n",
    "    from dataclasses import dataclass\n",
    "    from typing import List, Dict, Optional\n",
    "    import time\n",
    "    import logging\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(\"ollabridge-demo\")\n",
    "\n",
    "    @dataclass\n",
    "    class ChatConfig:\n",
    "        model: str = \"deepseek-r1\"\n",
    "        temperature: float = 0.2\n",
    "        max_retries: int = 2\n",
    "        request_timeout_s: int = 60  # SDK uses httpx; timeouts are handled internally\n",
    "\n",
    "    def chat_once(messages: List[Dict[str, str]], cfg: ChatConfig) -> str:\n",
    "        last_err: Optional[Exception] = None\n",
    "        for attempt in range(cfg.max_retries + 1):\n",
    "            try:\n",
    "                t0 = time.time()\n",
    "                resp = client.chat.completions.create(\n",
    "                    model=cfg.model,\n",
    "                    messages=messages,\n",
    "                    temperature=cfg.temperature,\n",
    "                )\n",
    "                dt = time.time() - t0\n",
    "                logger.info(\"chat completion ok (%.2fs)\", dt)\n",
    "                return resp.choices[0].message.content\n",
    "            except Exception as e:\n",
    "                last_err = e\n",
    "                logger.warning(\"attempt %d failed: %s\", attempt + 1, e)\n",
    "                if attempt < cfg.max_retries:\n",
    "                    time.sleep(1.5 * (attempt + 1))\n",
    "        raise RuntimeError(f\"All retries exhausted. Last error: {last_err}\") from last_err\n",
    "\n",
    "    print(\n",
    "        chat_once(\n",
    "            [{\"role\":\"user\",\"content\":\"Write a one-sentence definition of an API gateway.\"}],\n",
    "            ChatConfig(model=model_name),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7069374f",
   "metadata": {},
   "source": [
    "## 8) Troubleshooting checklist\n",
    "\n",
    "### Gateway not reachable\n",
    "- Confirm the process is running:\n",
    "  - `Uvicorn running on http://0.0.0.0:11435`\n",
    "- Verify the URL:\n",
    "  - Base: `http://localhost:11435/v1`\n",
    "  - Health: `http://localhost:11435/health`\n",
    "\n",
    "### 401 / unauthorized\n",
    "- Ensure you are using the correct key from the banner:\n",
    "  - `Authorization: Bearer sk-ollabridge-...`\n",
    "\n",
    "### Model not found\n",
    "- Verify the model name in your gateway banner, and set:\n",
    "  - `OLLAS_MODEL=...`\n",
    "\n",
    "### Dev workflow (auto-reload)\n",
    "- With `--reload`, edits to the code trigger an automatic server reload.\n",
    "  - Great for rapid iteration\n",
    "  - Avoid in production (use a fixed build + stable config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f6b310",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "- Add observability: request IDs, structured logs, metrics.\n",
    "- Use a reverse proxy (TLS termination) for shared environments.\n",
    "- Rotate keys and enforce allowlists/rate limits.\n",
    "\n",
    "If you want, I can also generate:\n",
    "- a small **FastAPI** client service that calls your gateway\n",
    "- a **Docker Compose** demo for local dev + reverse proxy + metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c8f17e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "451423ab",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "title": "OllaBridge Local Demo"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
