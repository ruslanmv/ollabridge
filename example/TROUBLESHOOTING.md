# ðŸ”§ Troubleshooting Guide

## The "Failed to fetch" Error

If you see this error in the browser:
```
> âŒ Connection failed: Failed to fetch
```

### Root Cause

The browser cannot connect to OllaBridge. This happens when:
1. **Ollama is not running** â† Most common cause!
2. **OllaBridge is not running**
3. **CORS is not configured correctly**
4. **Wrong URL or port**

---

## ðŸš€ Quick Fix (Recommended)

Run these commands in order:

```bash
cd example

# 1. Start all services automatically
make start

# 2. Run the demo
make run

# 3. Open http://localhost:3000 in your browser
```

That's it! The `make start` command will:
- âœ… Start Ollama
- âœ… Pull a model if needed
- âœ… Configure .env with correct CORS
- âœ… Start OllaBridge
- âœ… Verify everything is working

---

## ðŸ” Debug Step-by-Step

If `make start` doesn't work, debug manually:

### Step 1: Run the Debugger

```bash
make debug
```

This will test each component and tell you exactly what's wrong.

### Step 2: Fix Each Issue

Based on the debug output, fix the issues:

#### Issue: Ollama not running

**Fix:**
```bash
# Start Ollama in background
ollama serve &

# Or in a separate terminal
ollama serve
```

**Verify:**
```bash
curl http://localhost:11434/api/tags
```

#### Issue: No models available

**Fix:**
```bash
# Pull a model (first time only)
ollama pull llama3

# Verify
ollama list
```

#### Issue: OllaBridge not running

**Fix:**
```bash
# Start OllaBridge in background
cd /path/to/ollabridge
ollabridge start &

# Or in a separate terminal
ollabridge start
```

**Verify:**
```bash
curl http://localhost:11435/health
```

#### Issue: CORS not configured

**Fix:**
```bash
# Edit .env file in project root
nano .env

# Add or update this line:
CORS_ORIGINS=http://localhost:3000,http://localhost:5173,http://127.0.0.1:3000

# Restart OllaBridge
pkill -f ollabridge
ollabridge start &
```

**Verify:**
```bash
grep CORS_ORIGINS .env
```

---

## ðŸ“‹ Complete Checklist

Use this checklist to verify everything:

```bash
# In example/ directory
make check-setup
```

You should see all âœ…:

```
âœ… Ollama is installed
âœ… Ollama is running on port 11434
âœ… Models found
âœ… OllaBridge is installed
âœ… OllaBridge is running on port 11435
âœ… .env file exists
âœ… CORS includes localhost:3000
âœ… API key configured
```

---

## ðŸŽ¯ Test the Connection

Once all checks pass, test the actual connection:

```bash
# Run this from example/ directory
make debug
```

You should see:
```
âœ… Ollama is responding
âœ… OllaBridge is responding
âœ… Authentication working
âœ… CORS includes localhost:3000
âœ… Chat completion working!
```

---

## ðŸŒ Test in Browser

1. **Start the demo:**
   ```bash
   make run
   ```

2. **Open browser:**
   ```
   http://localhost:3000
   ```

3. **Click "Connect"**
   - Values should be auto-filled
   - Connection should succeed

4. **Send a test prompt:**
   ```
   What is the capital of Italy?
   ```

5. **Expected response:**
   ```
   Rome (or Roma)
   ```

---

## ðŸ› Common Issues

### Issue: "Cannot access from file:///"

**Problem:** Opening index.html directly in browser

**Fix:** Must serve via HTTP:
```bash
make run  # Serves on http://localhost:3000
```

### Issue: "CORS policy: No 'Access-Control-Allow-Origin'"

**Problem:** CORS not configured

**Fix:**
```bash
# Edit .env
CORS_ORIGINS=http://localhost:3000

# Restart OllaBridge
pkill -f ollabridge && ollabridge start &
```

### Issue: "Connection refused"

**Problem:** OllaBridge not running

**Fix:**
```bash
# Check if running
curl http://localhost:11435/health

# If not, start it
ollabridge start &
```

### Issue: "Model not found"

**Problem:** Requested model doesn't exist

**Fix:**
```bash
# List available models
ollama list

# Pull a model
ollama pull llama3

# Update DEFAULT_MODEL in .env if needed
```

---

## ðŸ›Ÿ Still Having Issues?

1. **Check logs:**
   ```bash
   tail -f /tmp/ollama.log
   tail -f /tmp/ollabridge.log
   ```

2. **Restart everything:**
   ```bash
   # Stop all services
   pkill -f ollama
   pkill -f ollabridge

   # Start fresh
   make start
   ```

3. **Verify ports:**
   ```bash
   # Check what's using the ports
   lsof -i :11434  # Ollama
   lsof -i :11435  # OllaBridge
   lsof -i :3000   # Demo client
   ```

4. **Check network:**
   ```bash
   # Can you reach localhost?
   ping localhost

   # Are ports accessible?
   telnet localhost 11434
   telnet localhost 11435
   ```

---

## ðŸ“ž Get Help

If you've tried everything above and it still doesn't work:

1. **Run full diagnostics:**
   ```bash
   make check-setup > diagnostics.txt
   make debug >> diagnostics.txt
   ```

2. **Share the output** in a GitHub issue

3. **Include:**
   - Operating system (WSL, Mac, Linux)
   - Python version (`python3 --version`)
   - Ollama version (`ollama --version`)
   - Contents of diagnostics.txt

---

## âœ… Success!

When everything works, you should be able to:

1. Run `make start` (starts all services)
2. Run `make run` (opens demo)
3. Click "Connect" in browser
4. Type: "What is the capital of Italy?"
5. Get response: "Rome" or "Roma"

**That's a fully working OllaBridge setup!** ðŸŽ‰
