# ================================================
# OllaBridge Configuration Template
# ================================================
# Copy this file to the project root as '.env' and customize the values
#
# Quick Start:
#   cp example/.env.example .env
#   nano .env  # Edit with your values
#   ollabridge start

# ------------------------------------------------
# Server Configuration
# ------------------------------------------------

# Host to bind the server to
# Use 0.0.0.0 to accept connections from any network interface
# Use 127.0.0.1 or localhost for local-only access
HOST=0.0.0.0

# Port for the OllaBridge server
# Default OpenAI API port is 11434, but we use 11435 to avoid conflicts with Ollama
PORT=11435

# ------------------------------------------------
# Authentication
# ------------------------------------------------

# Comma-separated list of valid API keys
# IMPORTANT: Change these in production!
#
# Generate secure keys with:
#   openssl rand -hex 32
#   python3 -c "import secrets; print(secrets.token_hex(32))"
#
# Examples:
#   API_KEYS=dev-key-change-me
#   API_KEYS=key1,key2,key3
#   API_KEYS=a1b2c3d4e5f6,x9y8z7w6v5u4
API_KEYS=dev-key-change-me

# ------------------------------------------------
# CORS (Cross-Origin Resource Sharing)
# ------------------------------------------------

# Comma-separated list of allowed origins for browser clients
# CRITICAL: Must include your client URL for web apps!
#
# Development examples:
#   CORS_ORIGINS=http://localhost:3000
#   CORS_ORIGINS=http://localhost:3000,http://localhost:5173
#
# Production examples:
#   CORS_ORIGINS=https://myapp.com,https://www.myapp.com
#   CORS_ORIGINS=https://app.example.com
CORS_ORIGINS=http://localhost:3000,http://localhost:5173

# ------------------------------------------------
# Ollama Backend Configuration
# ------------------------------------------------

# URL where Ollama is running
# Default: http://localhost:11434
#
# Examples:
#   OLLAMA_BASE_URL=http://localhost:11434      # Local Ollama
#   OLLAMA_BASE_URL=http://192.168.1.100:11434  # Remote Ollama on LAN
#   OLLAMA_BASE_URL=http://ollama:11434         # Docker container
OLLAMA_BASE_URL=http://localhost:11434

# ------------------------------------------------
# Default Model Configuration
# ------------------------------------------------

# Default model to use when client doesn't specify one
# Must be a model that exists in your Ollama installation
#
# Check available models: ollama list
# Pull new models: ollama pull <model-name>
#
# Popular models:
#   deepseek-r1    - DeepSeek R1 (reasoning model)
#   llama3         - Meta's Llama 3
#   mistral        - Mistral AI
#   codellama      - Code-specialized Llama
#   phi3           - Microsoft Phi-3
DEFAULT_MODEL=deepseek-r1

# ------------------------------------------------
# Optional: Logging Configuration
# ------------------------------------------------

# Log level (uncomment to enable)
# Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
# LOG_LEVEL=INFO

# ------------------------------------------------
# Optional: Advanced Settings
# ------------------------------------------------

# Maximum request timeout in seconds (uncomment to enable)
# REQUEST_TIMEOUT=300

# Enable/disable request logging (uncomment to enable)
# LOG_REQUESTS=true

# ------------------------------------------------
# Quick Reference
# ------------------------------------------------
#
# Start OllaBridge:
#   ollabridge start
#
# Stop OllaBridge:
#   ollabridge stop
#
# Check status:
#   curl http://localhost:11435/health
#
# List models:
#   curl -H "Authorization: Bearer dev-key-change-me" http://localhost:11435/v1/models
#
# Test chat completion:
#   curl -X POST http://localhost:11435/v1/chat/completions \
#     -H "Authorization: Bearer dev-key-change-me" \
#     -H "Content-Type: application/json" \
#     -d '{"model":"deepseek-r1","messages":[{"role":"user","content":"Hello!"}]}'
#
# ------------------------------------------------
