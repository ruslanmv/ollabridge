# üöÄ Quick Start Guide

## The Problem You're Seeing

```bash
curl: (7) Failed to connect to localhost port 11435 after 0 ms: Connection refused
```

**This means:** OllaBridge is not running. Nothing is listening on port 11435.

---

## ‚úÖ The Solution (Copy & Paste These Commands)

### On Your WSL Terminal:

```bash
# 1. Go to the example folder
cd /mnt/c/workspace/ollabridge/example

# 2. Start ALL services automatically
make start
```

**This will:**
- ‚úÖ Start Ollama on port 11434
- ‚úÖ Pull a model if you don't have one
- ‚úÖ Create/update .env with correct CORS settings
- ‚úÖ Start OllaBridge on port 11435
- ‚úÖ Verify everything is working

### After `make start` succeeds:

```bash
# 3. Run the demo
make run
```

### Then in your browser:

1. Open: **`http://localhost:3000`** (NOT 127.0.0.1:3000)
2. Click "Connect" (values are auto-filled)
3. Type: "What is the capital of Italy?"
4. Click "Send Prompt"
5. You should see: "Rome" ‚úÖ

---

## üîç If `make start` Fails

### Check what's failing:

```bash
make debug
```

This will show you exactly what's wrong.

### Common issues:

#### 1. Ollama not installed

**Error:** `Ollama is not installed`

**Fix:**
```bash
curl https://ollama.ai/install.sh | sh
```

#### 2. Python module not found

**Error:** `ModuleNotFoundError: No module named 'ollabridge'`

**Fix:**
```bash
cd /mnt/c/workspace/ollabridge
pip install -e .
```

#### 3. Port already in use

**Error:** `Address already in use`

**Fix:**
```bash
# Find what's using the port
lsof -i :11435
# Kill it
kill -9 <PID>
# Or change the port in .env
```

---

## üìã Step-by-Step Manual Setup (If Automatic Fails)

### Terminal 1: Start Ollama

```bash
ollama serve
```

Leave this running. You should see:
```
Listening on 127.0.0.1:11434
```

### Terminal 2: Pull a Model (if you haven't)

```bash
ollama pull llama3
```

### Terminal 3: Configure OllaBridge

```bash
cd /mnt/c/workspace/ollabridge

# Create .env if it doesn't exist
cat > .env << 'EOF'
# Server Configuration
HOST=0.0.0.0
PORT=11435

# Authentication
API_KEYS=dev-key-change-me

# CORS - CRITICAL for browser demo!
CORS_ORIGINS=http://localhost:3000,http://localhost:5173,http://127.0.0.1:3000

# Ollama Backend
OLLAMA_BASE_URL=http://localhost:11434

# Default Model
DEFAULT_MODEL=llama3
EOF
```

### Terminal 4: Start OllaBridge

```bash
cd /mnt/c/workspace/ollabridge
ollabridge start
```

You should see:
```
INFO:     Uvicorn running on http://0.0.0.0:11435
```

### Verify Services Are Running:

```bash
# Test Ollama
curl http://localhost:11434/api/tags

# Test OllaBridge
curl http://localhost:11435/health

# Test with API key
curl -H "Authorization: Bearer dev-key-change-me" \
  http://localhost:11435/v1/models
```

All should return JSON responses (not "Connection refused").

### Terminal 5: Run Demo

```bash
cd /mnt/c/workspace/ollabridge/example
make run
```

---

## üéØ Verify Everything Works

### From WSL terminal:

```bash
# This should work:
curl http://localhost:11435/health

# This should also work:
curl -H "Authorization: Bearer dev-key-change-me" \
  http://localhost:11435/v1/models
```

### From browser:

1. Open `http://localhost:3000` (exact URL!)
2. Click "Connect"
3. Should see "Connected ‚úÖ"
4. Send test: "What is the capital of Italy?"
5. Should get response: "Rome"

---

## ‚ö†Ô∏è Critical Points

### 1. URL Must Be EXACT

- ‚úÖ `http://localhost:3000` - Works
- ‚ùå `http://127.0.0.1:3000` - CORS error
- ‚ùå `file:///path/to/index.html` - Won't work

### 2. CORS Must Match

Your `.env` must include:
```env
CORS_ORIGINS=http://localhost:3000,http://localhost:5173,http://127.0.0.1:3000
```

If you change it, restart OllaBridge:
```bash
pkill -f ollabridge
ollabridge start
```

### 3. All 3 Services Must Run

| Service | Port | Check Command |
|---------|------|---------------|
| Ollama | 11434 | `curl http://localhost:11434/api/tags` |
| OllaBridge | 11435 | `curl http://localhost:11435/health` |
| Demo Client | 3000 | Open in browser |

---

## üõü Still Not Working?

### Run full diagnostics:

```bash
cd /mnt/c/workspace/ollabridge/example
make check-setup > ~/diagnostics.txt 2>&1
make debug >> ~/diagnostics.txt 2>&1
cat ~/diagnostics.txt
```

### Check logs:

```bash
# Ollama log
tail -f /tmp/ollama.log

# OllaBridge log
tail -f /tmp/ollabridge.log
```

### Common error messages:

#### "Failed to fetch"
- OllaBridge not running ‚Üí `make start`
- Wrong URL ‚Üí Use `http://localhost:3000`
- CORS issue ‚Üí Check `.env` CORS_ORIGINS

#### "Connection refused"
- Service not started ‚Üí `make start`
- Wrong port ‚Üí Check `.env` PORT setting
- Firewall blocking ‚Üí Check WSL firewall

#### "401 Unauthorized"
- Wrong API key ‚Üí Check `.env` API_KEYS
- Not using Bearer token ‚Üí Should be automatic

---

## üéâ Success Checklist

- [ ] `make start` completes without errors
- [ ] `curl http://localhost:11435/health` returns JSON
- [ ] `make run` starts without errors
- [ ] Browser opens `http://localhost:3000` (not 127.0.0.1)
- [ ] Click "Connect" ‚Üí Status shows "Connected ‚úÖ"
- [ ] Send prompt "What is the capital of Italy?"
- [ ] Get response "Rome" or "Roma"

**If all checked ‚úÖ - You're done!** üéâ

---

## üìû Get More Help

- Run: `make help` - See all available commands
- Read: `TROUBLESHOOTING.md` - Detailed problem solutions
- Check: example/README.md - Full documentation
