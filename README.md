<div align="center">

<img src="assets/logo.svg" alt="OllaBridge Logo" width="500"/>

# OllaBridge âš¡ï¸

**Your single gateway to ALL your LLMs â€” local, remote, anywhere.**

[![PyPI version](https://badge.fury.io/py/ollabridge.svg)](https://badge.fury.io/py/ollabridge)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

[Quick Start](#-60-second-start) â€¢ [Why OllaBridge](#-why-ollabridge) â€¢ [Distributed Compute](#-add-any-gpu-in-60-seconds) â€¢ [Examples](#-use-it-anywhere) â€¢ [MCP Mode](#-ai-agents-love-ollabridge)

</div>

---

## ğŸ¯ What is OllaBridge?

> **One gateway. All your LLMs. Everywhere.**

OllaBridge is your **single, OpenAI-compatible API** for every LLM you run â€” on your laptop, workstation, free GPU servers, cloud instances, anywhere.

**The Problem:** You have models running everywhere (laptop, cloud GPU, friend's gaming PC), and every app needs different configs.

**OllaBridge Solution:** Apps connect to ONE place. OllaBridge routes to the right compute automatically.

```mermaid
graph TB
    A[Your Apps] -->|OpenAI SDK| B[OllaBridge<br/>Control Plane]

    B -->|Auto Routes| C[Local Laptop<br/>llama3.1]
    B -->|Auto Routes| D[Free GPU Cloud<br/>deepseek-r1]
    B -->|Auto Routes| E[Remote Workstation<br/>mixtral]

    C -.->|Dials Out| B
    D -.->|Dials Out| B
    E -.->|Dials Out| B

    style B fill:#6366f1,stroke:#4f46e5,stroke-width:3px,color:#fff
    style A fill:#10b981,stroke:#059669,stroke-width:2px,color:#fff
    style C fill:#8b5cf6,stroke:#7c3aed,stroke-width:2px,color:#fff
    style D fill:#ec4899,stroke:#db2777,stroke-width:2px,color:#fff
    style E fill:#f59e0b,stroke:#d97706,stroke-width:2px,color:#fff
```

**Key Innovation:** Compute nodes **dial out** to your gateway. No port forwarding, no VPN, no config hell.

---

## ğŸš€ Why OllaBridge?

### ğŸ¯ **Single Source of Truth**
- âœ… **One URL for everything** â€” Your apps never change code
- âœ… **Zero config** â€” Add new GPUs without touching your app
- âœ… **Smart routing** â€” OllaBridge picks the best node automatically
- âœ… **OpenAI compatible** â€” Works with any SDK, framework, or tool

### ğŸ›¡ï¸ **Enterprise-Grade Security**
- âœ… **API key authentication** â€” Protect your LLMs
- âœ… **Rate limiting** â€” Control usage per key
- âœ… **Request logging** â€” Full audit trail
- âœ… **Encrypted connections** â€” TLS for remote nodes

### ğŸŒ **Works Everywhere**
- âœ… **Free GPU clouds** â€” Colab, Kaggle, Lightning AI (no port forwarding needed!)
- âœ… **Ephemeral instances** â€” Nodes dial out, IPs don't matter
- âœ… **Behind firewalls** â€” Your laptop can join from coffee shop WiFi
- âœ… **Mixed environments** â€” Combine local + cloud seamlessly

### ğŸ¤– **AI Agent Ready**
- âœ… **MCP server** â€” Agents can control your infrastructure
- âœ… **Tool exposure** â€” Manage nodes, routes, health via tools
- âœ… **Self-healing** â€” Auto-install, auto-configure, auto-recover

---

## âš¡ 60-Second Start

### Step 1: Install

```bash
pip install ollabridge
```

### Step 2: Start Your Gateway

```bash
ollabridge start
```

**That's it!** You'll see:

```
âœ… Ollama installed (if needed)
âœ… Model downloaded (if needed)
âœ… Gateway online at http://localhost:11435

â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ğŸš€ Gateway Ready â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚                                                          â”‚
â”‚ âœ… OllaBridge is Online                                  â”‚
â”‚                                                          â”‚
â”‚ Model:        deepseek-r1                                â”‚
â”‚ Local API:    http://localhost:11435/v1                 â”‚
â”‚ Key:          sk-ollabridge-xY9kL2mN8pQ4rT6vW1zA        â”‚
â”‚                                                          â”‚
â”‚ Node join token:  eyJ0eXAi...                           â”‚
â”‚ Example node command:                                    â”‚
â”‚   ollabridge-node join --control http://localhost:11435 â”‚
â”‚                        --token eyJ0eXAi...              â”‚
â”‚                                                          â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
```

### Step 3: Use It!

```python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:11435/v1",
    api_key="sk-ollabridge-xY9kL2mN8pQ4rT6vW1zA"
)

response = client.chat.completions.create(
    model="deepseek-r1",
    messages=[{"role": "user", "content": "Hello!"}]
)

print(response.choices[0].message.content)
```

**Done!** You're running private LLMs with the OpenAI API.

---

## ğŸŒ Add Any GPU in 60 Seconds

Have a free GPU on Colab? A remote workstation? Add it instantly:

### On Your Remote GPU/Machine:

```bash
# Install
pip install ollabridge

# Join your gateway (copy the command from gateway startup)
ollabridge-node join \
  --control http://YOUR_GATEWAY_IP:11435 \
  --token eyJ0eXAi...
```

**That's it!** The remote GPU:
- âœ… Auto-installs Ollama if needed
- âœ… Auto-downloads models if needed
- âœ… **Dials out** to your gateway (no port forwarding!)
- âœ… Shows up as available compute

### Your Apps See It Automatically

```python
# Same code, now uses both local + remote GPU!
client = OpenAI(base_url="http://localhost:11435/v1", ...)
response = client.chat.completions.create(...)  # Auto-routed
```

**OllaBridge routes requests** across all your nodes automatically.

---

## ğŸ¯ Real-World Scenarios

### Scenario 1: "I have a gaming PC at home"

```bash
# On your gaming PC:
ollabridge-node join --control https://your-gateway.com --token ...

# Now your laptop can use your gaming PC's GPU
# Even if you're at a coffee shop!
```

### Scenario 2: "I want to use free Colab GPUs"

```python
# In Colab notebook:
!pip install ollabridge
!ollabridge-node join --control https://your-gateway.com --token ...

# Now your production app can use free Colab compute
# Colab session ends? Start a new one. Zero config changes.
```

### Scenario 3: "I have multiple cloud GPUs"

```bash
# Each GPU instance:
ollabridge-node join --control https://gateway.company.com --token ...

# Your team shares one API URL
# OllaBridge load-balances across all GPUs
```

---

## ğŸ’» Use It Anywhere

### Python (OpenAI SDK)

```python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:11435/v1",
    api_key="your-key-here"
)

# Chat
response = client.chat.completions.create(
    model="deepseek-r1",
    messages=[{"role": "user", "content": "Explain quantum computing"}]
)

# Embeddings
embeddings = client.embeddings.create(
    model="nomic-embed-text",
    input="Hello, world!"
)
```

### Node.js / TypeScript

```typescript
import OpenAI from "openai";

const client = new OpenAI({
  baseURL: "http://localhost:11435/v1",
  apiKey: process.env.OLLABRIDGE_KEY
});

const completion = await client.chat.completions.create({
  model: "deepseek-r1",
  messages: [{ role: "user", content: "Hello!" }]
});
```

### LangChain

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(
    base_url="http://localhost:11435/v1",
    api_key="your-key-here",
    model="deepseek-r1"
)

response = llm.invoke("What is the meaning of life?")
```

### cURL

```bash
curl -X POST http://localhost:11435/v1/chat/completions \
  -H "Authorization: Bearer your-key-here" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "deepseek-r1",
    "messages": [{"role": "user", "content": "Hello!"}]
  }'
```

**Works with ANY OpenAI-compatible tool or library.**

---

## ğŸ¤– AI Agents Love OllaBridge

OllaBridge has a **Model Context Protocol (MCP) server** built-in.

Agents can:
- âœ… Create enrollment tokens
- âœ… List connected compute nodes
- âœ… Check gateway health
- âœ… Manage your LLM infrastructure via tools

### Start MCP Server

```bash
ollabridge-mcp
```

### Example: Agent Workflow

```python
# Agent can call these tools:
await session.call_tool("ollabridge.enroll.create", {})
# â†’ Returns enrollment token

await session.call_tool("ollabridge.runtimes.list", {})
# â†’ Shows all connected nodes

await session.call_tool("ollabridge.gateway.health", {})
# â†’ Checks gateway status
```

**Use Case:** "Hey Claude, add my workstation's GPU to our LLM gateway"

â†’ Agent creates token, gives you the command, you run it. Done.

---

## ğŸ” Security & Configuration

### Authentication

OllaBridge auto-generates a secure API key on first run (saved in `.env`):

```env
API_KEYS=sk-ollabridge-xY9kL2mN8pQ4rT6vW1zA
```

Use it in your apps:

```python
# Option 1: Bearer token
headers = {"Authorization": "Bearer sk-ollabridge-..."}

# Option 2: Custom header
headers = {"X-API-Key": "sk-ollabridge-..."}
```

### Configuration (`.env`)

```env
# API Keys (comma-separated for multiple)
API_KEYS=sk-ollabridge-abc123,sk-ollabridge-def456

# Server
HOST=0.0.0.0
PORT=11435

# Default models
DEFAULT_MODEL=deepseek-r1
DEFAULT_EMBED_MODEL=nomic-embed-text

# Rate limiting
RATE_LIMIT=60/minute

# Security
ENROLLMENT_SECRET=your-secret-here
ENROLLMENT_TTL_SECONDS=3600

# Database (optional)
DATABASE_URL=postgresql://user:pass@localhost/ollabridge
```

### Enrollment Tokens

Create short-lived tokens for nodes to join:

```bash
ollabridge enroll-create --ttl 3600
```

Tokens expire automatically for security.

---

## ğŸ“¡ API Reference

### Core Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/health` | GET | Gateway health + node count |
| `/v1/chat/completions` | POST | OpenAI-compatible chat |
| `/v1/embeddings` | POST | Generate embeddings |
| `/v1/models` | GET | List available models (aggregated from nodes) |

### Admin Endpoints (require API key)

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/admin/recent` | GET | Recent request logs |
| `/admin/runtimes` | GET | List connected nodes |
| `/admin/enroll` | POST | Create enrollment token |

### Example: Check Connected Nodes

```bash
curl -H "X-API-Key: your-key" http://localhost:11435/admin/runtimes
```

**Response:**
```json
{
  "runtimes": [
    {
      "node_id": "local",
      "connector": "local_ollama",
      "healthy": true,
      "tags": ["local"],
      "models": ["deepseek-r1", "llama3.1"]
    },
    {
      "node_id": "colab-gpu-1",
      "connector": "relay_link",
      "healthy": true,
      "tags": ["gpu", "free"],
      "models": ["mixtral", "codellama"]
    }
  ]
}
```

---

## ğŸ—ï¸ Architecture Deep Dive

### How It Works

1. **Control Plane (Gateway)**: Your apps connect here
2. **Nodes**: Any machine with GPUs/CPUs running models
3. **Relay Link**: Nodes dial OUT to gateway (WebSocket)
4. **Router**: Picks the best node for each request

### Why "Dial Out" Matters

**Traditional (broken):**
```
App â†’ Gateway â†’ Try to reach GPU
                âŒ Blocked by firewall
                âŒ NAT issues
                âŒ No public IP
```

**OllaBridge (works everywhere):**
```
App â†’ Gateway â† GPU dials in
               âœ… Works from anywhere
               âœ… No port forwarding
               âœ… Ephemeral IPs OK
```

### Connector Types

- **RelayLink**: Node dials out via WebSocket (default, works everywhere)
- **DirectEndpoint**: HTTP to stable node (best performance)
- **LocalOllama**: Built-in local runtime (zero config)

OllaBridge picks the right one automatically.

---

## ğŸ“ˆ Scaling

### Add More Workers

```bash
ollabridge start --workers 4
```

### Use PostgreSQL

```bash
pip install psycopg2-binary
export DATABASE_URL=postgresql://user:pass@localhost/ollabridge
ollabridge start --workers 8
```

### Add More Nodes

```bash
# Just keep adding nodes!
ollabridge-node join --control ... --token ...
```

OllaBridge automatically load-balances across all healthy nodes.

---

## ğŸŒ Public Access (Optional)

### Quick Demo (Ngrok)

```bash
ollabridge start --share
```

### Production (Cloudflare Tunnel)

```bash
# Terminal 1: Start gateway
ollabridge start

# Terminal 2: Expose it
cloudflared tunnel --url http://localhost:11435
```

Now your gateway has a public `https://` URL!

**Security:** Always use API keys for public gateways.

---

## ğŸ“ Beginner's Guide

### "I've never used LLMs before"

1. Install: `pip install ollabridge`
2. Start: `ollabridge start`
3. Copy the API key from the output
4. Use this code:

```python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:11435/v1",
    api_key="PASTE_KEY_HERE"
)

response = client.chat.completions.create(
    model="deepseek-r1",
    messages=[{"role": "user", "content": "Explain Python in simple terms"}]
)

print(response.choices[0].message.content)
```

**That's it!** You're running AI models on your computer.

### "I want to add my gaming PC's GPU"

1. On your main computer (gateway):
   ```bash
   ollabridge start
   # Copy the "Node join token" and gateway URL
   ```

2. On your gaming PC:
   ```bash
   pip install ollabridge
   ollabridge-node join --control http://GATEWAY_IP:11435 --token TOKEN_HERE
   ```

3. Done! Your apps can now use your gaming PC's power.

### "I want to use free Colab GPUs"

1. Start your gateway at home:
   ```bash
   ollabridge start --share
   # Note the public URL (https://xxx.ngrok.io)
   ```

2. In Colab notebook:
   ```python
   !pip install ollabridge
   !ollabridge-node join --control https://xxx.ngrok.io --token YOUR_TOKEN
   ```

3. Now your apps use FREE Colab GPUs!

**Pro tip:** When Colab disconnects, just restart and run step 2 again. Zero config changes needed.

---

## ğŸ› ï¸ Common Tasks

### List Available Models

```bash
curl http://localhost:11435/v1/models
```

### Check Gateway Health

```bash
curl http://localhost:11435/health
```

### See Connected Nodes

```bash
curl -H "X-API-Key: your-key" http://localhost:11435/admin/runtimes
```

### Create New Enrollment Token

```bash
ollabridge enroll-create
```

### View Recent Requests

```bash
curl -H "X-API-Key: your-key" http://localhost:11435/admin/recent
```

---

## ğŸ—ºï¸ Roadmap

- [x] âœ… Control Plane + Node architecture
- [x] âœ… Outbound-only node enrollment (no port forwarding)
- [x] âœ… MCP server for AI agent control
- [x] âœ… Multi-node load balancing
- [ ] ğŸš§ Tag-based routing (send "coding" requests to GPU nodes)
- [ ] ğŸš§ Model-specific routing rules
- [ ] ğŸš§ Streaming support for chat completions
- [ ] ğŸš§ Web UI for node management
- [ ] ğŸš§ Prometheus metrics
- [ ] ğŸš§ Support for more runtimes (vLLM, llama.cpp, LM Studio)

---

## ğŸ¤ Contributing

We welcome contributions! Areas we'd love help:

- ğŸ”Œ More runtime adapters (vLLM, llama.cpp, etc.)
- ğŸ¨ Web UI for management
- ğŸ“Š Better monitoring/metrics
- ğŸ”’ Security enhancements
- ğŸ“– Documentation improvements

**How to contribute:**

1. Fork the repo
2. Create a branch (`git checkout -b feature/amazing`)
3. Make your changes
4. Add tests
5. Submit a PR

---

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE)

---

## ğŸ™ Built With

- [FastAPI](https://fastapi.tiangolo.com/) â€” Modern async web framework
- [Ollama](https://ollama.ai/) â€” Run LLMs locally
- [WebSockets](https://websockets.readthedocs.io/) â€” Real-time node connections
- [SQLModel](https://sqlmodel.tiangolo.com/) â€” Database with Python types

---

## ğŸ’¬ Support

- ğŸ“– [Documentation](docs/)
- ğŸ› [Report Bug](https://github.com/ruslanmv/ollabridge/issues)
- ğŸ’¡ [Request Feature](https://github.com/ruslanmv/ollabridge/issues)
- ğŸ’¬ [Discussions](https://github.com/ruslanmv/ollabridge/discussions)

---

## ğŸŒŸ Star History

If OllaBridge helped you, give it a star! â­

---

<div align="center">

**Made with â¤ï¸ for the local-first AI community**

**Stop paying cloud tokens. Use your own compute.**

</div>
